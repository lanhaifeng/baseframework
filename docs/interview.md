1.java基础    
集合：Set、List、Map 

Set:HashSet、LinkedHashSet、TreeSet、CopyOnWriteArraySet、SynchronizedSet   

List:ArrayList、LinkedList、Vector、Stack、CopyOnWriteArrayList、SynchronizedList    

Map:HashMap、LinkedHashMap、TreeMap、Hashtable、ConcurrentHashMap   


HashSet:非线程安全，无序，底层包装了一个HashMap去实现的 
1.采用HashCode算法(既使用元素作为key)来存取集合中的元素，因此具有比较好的读取和查找功能
2.它不允许出现重复元素，不保证集合中的元素的顺序，允许包含值为null的元素，但最多只能有null元素

TreeSet：非线程安全，可排序，实现了SortedSet接口，底层使用TreeMap实现的，本质上是一个红黑树原理 
1.它在将对象元素添加到集合中会自动按照某种比较规则将其插入到有序的对象
序列中，并保证该集合元素组成的读序列按照升序排列
2.排序分为两种类型，一种是自然排序，另一种是定制排序

CopyOnWriteArraySet：线程安全，底层通过CopyOnWriteArrayList属性实现，利用addIfAbsent方法确保不重复

SynchronizedSet：线程安全，通过synchronized代码块儿，将传入的set变为线程安全

LinkedHashSet：非线程安全，有序，底层包装了一个LinkedHashMap去实现的

ArrayList：非线程安全，有序，长度可变的数组，可以对元素进行随机的访问，向ArrayList中插入与删除元素的速度慢

LinkedList：非线程安全，有序，采用链表数据结构，插入和删除速度快，但访问速度慢。

Vector：线程安全，长度可变的数组,方法上加synchronized关键字

Stack：线程安全，长度可变的数组,方法上加synchronized关键字

CopyOnWriteArrayList：线程安全，通过可变数组实现，写的时候通过ReentrantLock加锁，读的时候无锁

SynchronizedList：线程安全，通过synchronized代码块儿，将传入的list变为线程安全

Hashtable：线程安全，效率比较低，线程安全是通过在方法上加synchronized关键字

HashMap：非线程安全，底层是基于数组实现，每个数组元素又是单向链表或红黑树，
无序
HashMap扩容
1.map数据个数超过容量*0.75(默认)扩容
2.单一链表数据个数超过8个，而总容量小于64扩容
3.单一链表数据个数超过8个，而总容量超过64将当且链表转为TreeNode(红黑树)

LinkedHashMap：线程不安全的，继承于HashMap，是基于HashMap和双向链表来实现的
1.有序，可分为插入顺序和访问顺序两种
2.访问顺序，那put和get操作已存在的Entry时，都会把Entry移动到双向链表的
表尾(其实是先删除再插入)

TreeMap：线程不安全的，所有的元素都保持着某种固定的顺序,基于红黑树实现,
1.没有调优选项，因为该树总处于平衡状态
2.适用于按自然顺序或自定义顺序遍历键

jdk1.7ConcurrentHashMap：线程安全，由一个Segment数组构成，一个Segment
由一个HashEntry数组构成，HashEntry元素又组成一个单向链表
1.ConcurrentHashMap是一个二级哈希表，在一个总的哈希表下面，有若干个子哈希表
2.采用了锁分段技术，每一个Segment就好比一个自治区，读写操作高度自治，Segment之间互不影响
3.操作先得到hash值，再通过hash值得到Segment，再通过hash与长度减1得到
Segment中HashEntry，再遍历HashEntry链表找到对应的值

jdk1.8ConcurrentHashMap：线程安全，底层是基于数组实现，每个数组元素又是
单向链表或红黑树包装类，进行过优化，效率比较高
1.通过可见sizeCtl标识位无锁CAS保证初始化线程安全
2.空链表时，通过unsafe初始化表头
3.对于同一个数组元素的操作，通过synchronized代码块实现，put可能会很频繁，
如果自旋一直不成功，将会一直占用CPU，而且数组中其他的元素是可以正常访问的


字符串：String、StringBuilder、StringBuffer

ThreadLocal源码和设计原理

2.jvm内存结构、调优、垃圾回收机制
内存模型   

程序计数器(线程私有)：
是当前线程锁执行字节码的行号治时期，每条线程都有一个独立的程序计数器，这类内存也称为“线程私有”的内存。正在执行java方法的话，计数器记录的是虚拟机字节码指令的地址(当前指令的地址)。如果是Natice方法，则为空。

java虚拟机栈
也是线程私有的。
每个方法在执行的时候也会创建一个栈帧，存储了局部变量，操作数，动态链接，方法返回地址。
每个方法从调用到执行完毕，对应一个栈帧在虚拟机栈中的入栈和出栈。
通常所说的栈，一般是指在虚拟机栈中的局部变量部分。
局部变量所需内存在编译期间完成分配，
如果线程请求的栈深度大于虚拟机所允许的深度，则StackOverflowError。
如果虚拟机栈可以动态扩展，扩展到无法申请足够的内存，则OutOfMemoryError。

本地方法栈（线程私有）
和虚拟机栈类似，主要为虚拟机使用到的Native方法服务。也会抛出StackOverflowError 和OutOfMemoryError。

Java堆（线程共享）
被所有线程共享的一块内存区域，在虚拟机启动的时候创建，用于存放对象实例。
对可以按照可扩展来实现（通过-Xmx 和-Xms 来控制）
当队中没有内存可分配给实例，也无法再扩展时，则抛出OutOfMemoryError异常。

方法区（线程共享）
被所有方法线程共享的一块内存区域。
用于存储已经被虚拟机加载的类信息，常量，静态变量等。
这个区域的内存回收目标主要针对常量池的回收和堆类型的卸载。

元空间
在JDK 1.8中，HotSpot虚拟机设计团队为了促进HotSpot与 JRockit的融合，修改了方法区的实现，移除了永久代，
并且将原来放在方法区的字符串常量池和静态变量都转移到了Java堆中，选择使用本地化的内存空间(
而不是JVM的内存空间)存放类的元数据，这个空间叫做元空间(Metaspace)。

做了这个改动以后，java.lang.OutOfMemoryError: PermGen的空间问题将不复存在，并且不再需要调整和监控这个内存空间。且虚拟机需要为方法区
设计额外的GC策略：如果类元数据的空间占用达到参数“MaxMetaspaceSize”设置的值，将会触发对死亡对象和类加载器的垃圾回收。为了限制垃圾回收的
频率和延迟，适当的监控和调优元空间是非常有必要的。元空间过多的垃圾收集可能表示类、类加载器内存泄漏或对你的应用程序来说空间太小了。

元空间的内存管理由元空间虚拟机来完成。先前，对于类的元数据我们需要不同的垃圾回收器进行处理，现在只需要执行元空间虚拟机的C++代码即可完成。
在元空间中，类和其元数据的生命周期和其对应的类加载器是相同的。话句话说，只要类加载器存活，其加载的类的元数据也是存活的，因而不会被回收掉。

准确的来说，每一个类加载器的存储区域都称作一个元空间，所有的元空间合在一起就是我们一直说的元
空间。当一个类加载器被垃圾回收器标记为不再存活，其对应的元空间会被回收。在元空间的回收过程中没有重定位和压缩等操作。但是元空间内的元数据
会进行扫描来确定Java引用。

直接内存
直接内存位于本地内存，不属于JVM内存，但是也会在物理内存耗尽的时候报OOM。

在jdk1.4中加入了NIO(New Input/Putput)类，引入了一种基于通道(channel)与缓冲区(buffer)的新IO方式，
它可以使用native函数直接分配堆外内存，然后通过存储在java堆中的DirectByteBuffer对象作为这块内存的引
用进行操作，这样可以在一些场景下大大提高IO性能，避免了在java堆和native堆来回复制数据。

本机直接内存的分配不会受到Java堆大小的限制，但是既然是内存，还是会受到本机总内存(包括RAM以及SWAP区或分页文件)
大小以及处理器寻址空间的限制。服务器管理员在配置虚拟机参数时，会根据实际内存设置-Xmx等参数信息，但经常忽略
直接内存，使得各个内存区域总和大于物理内存限制(包括物理的和操作系统的限制)，从而导致动态扩展时出现OutOfMemoryError异常。

直接内存分类
+ Netty DirectBuffer，受MaxDirectMemorySize参数限制
+ JNI、JNA操纵本地内存
+ Unasfe

直接内存优点
+ 减少了垃圾回收的工作，垃圾回收会暂停其他的工作
+ 加快了复制的速度。堆内在flush到远程时，需要先复制到直接内存，在发送
+ 可以在进程之间共享，减少jvm间对象复制，使得jvm的分割部署更容易实现
+ 可以扩展至更大的内存空间
+ DirectBuffer读写操作比普通buffer快接近一倍，创建、销毁话费时间接近普通buffer20倍

直接内存缺点
+ 直接内存难以控制，内存泄漏，很难排查
+ 直接内存相对而言，不适合存储复杂的对象

直接内存使用场景
+ 有很大的数据需要存储，它的生命周期很长
+ 适合频繁的IO操作，例如网络并发场景

字符串常量池的设计思想
  1.字符串的分配，和其他的对象分配一样，耗费高昂的时间与空间代价，作为最基础的数据类型，
大量频繁的创建字符串，极大程度地影响程序的性能

  2.JVM为了提高性能和减少内存开销，在实例化字符串常量的时候进行了一些优化
     2.1为字符串开辟一个字符串常量池，类似于缓存区
     2.2创建字符串常量时，首先检测字符串常量池是否存在该字符串
     2.3存在该字符串，返回引用实例，不存在，实例化该字符串并放入池中

  3.实现的基础
     3.1实现该优化的基础是因为字符串是不可变的，可以不用担心数据冲突进行共享
     3.2运行时实时创建的全局字符串常量池中有一个表，总是为池中每个唯一的字符串对象维护一个引用,
        这就意味着它们一直引用着字符串常量池中的对象，所以，在常量池中的这些字符串不会被垃圾收集器回收


垃圾回收机制

垃圾回收算法


垃圾回收器


3.数据索引结构
b-tree、

4.多线程、线程池、锁(悲观、乐观、公平、非公平、独占锁、共享锁)、CAS、AQS
悲观锁：悲观锁，正如其名，具有强烈的独占和排他特性。它指的是对数据被外界(包括本系统当前的其他事务，以及来自外部系统的事务处理)修改持保守态度。因此，在整个数据处理过程中，将数据处于锁定状态

悲观锁主要分为共享锁和排他锁
共享锁【shared locks】又称为读锁，简称S锁。顾名思义，共享锁就是多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改。
排他锁【exclusive locks】又称为写锁，简称X锁。顾名思义，排他锁就是不能与其他锁并存，如果一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的其他锁，包括共享锁和排他锁，但是获取排他锁的事务是可以对数据行读取和修改

悲观锁的实现
1.传统的关系型数据库使用这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁
2.Java里面的同步synchronized关键字的实现

乐观锁：乐观锁假设数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则返回给用户错误的信息，让用户决定如何去做。乐观锁适用于读操作多的场景，这样可以提高程序的吞吐量

乐观锁的实现
1.CAS 实现：Java 中java.util.concurrent.atomic包下面的原子变量使用了乐观锁的一种CAS实现方式
2.版本控制：一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数。当数据被修改时，version 值会+1。
当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值与当前数据库中的version值相等时才更新，
否则重试更新操作，直到更新成功

CAS的ABA问题：
1.比如说线程一从数据库中取出库存数 3，这时候线程二也从数据库中取出库存数 3，并且线程二进行了一些操作变成了 2。
2.然后线程二又将库存数变成 3，这时候线程一进行CAS操作发现数据库中仍然是 3，然后线程一操作成功。
3.尽管线程一的 CAS 操作成功，但是不代表这个过程就是没有问题的
一个比较好的解决办法，就是通过一个单独的可以顺序递增的version字段或时间戳，如AtomicStampedReference

CAS消耗资源：
多个线程争夺同一个资源时，如果自旋一直不成功，将会一直占用CPU
解决方法：破坏掉for死循环，当超过一定时间或者一定次数时，return退出。
JDK8新增的LongAddr,和ConcurrentHashMap类似的方法。当多个线程竞争时，
将粒度变小，将一个变量拆分为多个变量，达到多个线程访问多个资源的效果，
最后再调用sum把它合起来。

CAS多变量共享一致性问题：
解决方法： CAS操作是针对一个变量的，如果对多个变量操作，
1)可以加锁来解决。
2)封装成对象类解决。


AQS原理


Synchronized原理、优化、锁膨胀
原理
JVM 是通过进入、退出对象监视器(Monitor)来实现对方法、同步块的同步的，而对象监视器的本质依赖于底层
操作系统的互斥锁(Mutex Lock)实现。

具体实现是在编译之后在同步方法调用前加入一个monitor.enter指令，在退出方法和异常处插入monitor.exit
的指令。

对于没有获取到锁的线程将会阻塞到方法入口处，直到获取锁的线程monitor.exit之后才能尝试继续获取锁。

同步块的入口和出口分别有monitorenter和monitorexit指令。当执行monitorenter指令时，线程试图获取
锁也就是获取monitor（monitor对象存在于每个Java对象的对象头中，synchronized锁便是通过这种方式获取锁的，也是为什么Java中任意对象可以作为锁的原因）的持有权。当计数器为0则可以成功获取，获取后将锁计数器设为1也就是加1。相应的在执行monitorexit指令后，将锁计数器设为0，表明锁被释放。如果获取对象锁失败，那当前线程就要阻塞等待，直到锁被另外一个线程释放为止。

在synchronized修饰方法时是添加ACC_SYNCHRONIZED标识，该标识指明了该方法是一个同步方法，JVM通过该
ACC_SYNCHRONIZED访问标志来辨别一个方法是否声明为同步方法，从而执行相应的同步调用

Synchronized属性
（1）原子性：确保线程互斥的访问同步代码；
为什么volatile已经保证了变量的可见性，synchronized依然保证变量的可见性：
（2）可见性：保证共享变量的同步能够及时可见，通过JMM模型：“对一个共享变量unlock（解锁）之前一定要先同步到主内存中；如果要是对一个共享变量进行lock（锁定）的之前一定要清除工作内存中的值，然后通过主内存中的值加载到工作内存中”；
（3）有序性：有效解决重排序问题，即 “一个unlock操作先行发生(happen-before)于后面对同一个锁的lock操作”；

https://blog.csdn.net/weixin_36759405/article/details/83034386


多线程返回结果
Runnable、Callable、Future、FutureTask、CompletionService

CountDownLatch是一个同步辅助类，它允许一个或多个线程一直等待直到其他线程执行完毕才开始执行。
用给定的计数初始化CountDownLatch，其含义是要被等待执行完的线程个数。
每次调用CountDown()，计数减1
主程序执行到await()函数会阻塞等待线程的执行，直到计数为0
计数器通过使用锁（共享锁、排它锁AQS）实现


https://blog.csdn.net/jdsjlzx/article/details/52912701

单核、多核CPU的原子操作
软件级别的原子操作是依赖于硬件支持的

https://www.cnblogs.com/javaleon/p/4292656.html

5.高并发
垂直扩展：提升单机处理能力
5.1增强单机硬件性能，例如：增加CPU核数如32核，升级更好的网卡如万兆，升级更好的硬盘如SSD，扩充硬盘容量如2T，扩充系统内存如128G；
5.2提升单机架构性能，例如：使用Cache来减少IO次数，使用异步来增加单服务吞吐量，使用无锁数据结构来减少响应时间；


水平扩展：只要增加服务器数量，就能线性扩充系统性能。水平扩展对系统架构设计是有要求的，如何在架构各层进行可水平扩展的设计，以及互联网公司架构各层常见的水平扩展实践，是本文重点讨论的内容
nginx分流+集群

6.分布式、集群
分布式事务锁

7.高可用

8.sql优化分库、分表
cpu瓶颈：索引优化sql、单表数据太大水平分表
磁盘IO瓶颈：热点数据太多，数据库缓存放不下，每次查询时会产生大量的IO，降低查询速度 -> 分库和垂直分表
网络IO瓶颈：请求的数据太多，网络带宽不够 -> 分库

索引时
1.insert变慢
2.select正常走索引变快
3.update，如果set索引字段变慢，如果where走索引变快
4.delete要看where条件后走索引节省的时间与删除索引花费时间的差值了

水平
场景：系统绝对并发量上来了，分表难以根本上解决问题，并且还没有明显的业务归属来垂直分库
水平分库：以字段为依据，按照一定策略（hash、range等），将一个库中的数据拆分到多个库中

场景：系统绝对并发量并没有上来，只是单表的数据量太多，影响了SQL效率，加重了CPU负担，以至于成为瓶颈
水平分表：以字段为依据，按照一定策略（hash、range等），将一个表中的数据拆分到多个表中。


垂直
场景：系统绝对并发量上来了，并且可以抽象出单独的业务模块。
垂直分库：以表为依据，按照业务归属不同，将不同的表拆分到不同的库中

场景：系统绝对并发量并没有上来，表的记录并不多，但是字段多，并且热点数据和非热点数据在一起，单行数据所需的存储空间较大。以至于数据库缓存的数据行减少，查询时会去读磁盘数据产生大量的随机读IO，产生IO瓶颈
垂直分表：以字段为依据，按照字段的活跃性，将表中字段拆到不同的表（主表和扩展表）中


https://zhuanlan.zhihu.com/p/137368446

中间层代理类中间件
部署一台代理服务器伪装成 MySQL 服务器，代理服务器负责与真实 MySQL 节点的对接，应用程序只和代理服务器对接
远程代理：MyCat

应用层依赖类中间件
这类分库分表中间件的特点就是和应用强耦合，需要应用显示依赖相应的jar包，对代码有侵入性
jdbc分布分表：shardingsphere、Sharding-JDBC

https://blog.csdn.net/fly910905/article/details/87101059


9.http、https、证书、对称加密、非对称加密
+ 私钥加密、签名，公钥解密、验证签名
+ 证书可靠性保证：
  1.是否信任根证书链
  2.认为根证书私钥不会泄露
  3.用根证书的公钥验证权威机构颁发证书签名（即权威机构用私钥加密证书摘要算法的内容是否能被根证书公钥解密）
  
  
10.API序列化与反序列化
  jackson忽略字段的三种方式：
    1.@JsonIgnore
    2.@JsonIgnoreProperties
    3.@JsonIgnoreType
        
  动态过滤字段：@JsonFilter("myFilter")+过滤器
  
  条件序列化：自定义序列化类(extends JsonSerializer)
  
  
11.redis
单线程与多线程问题：
1.多线程能够提高IO和CPU利用率，而CPU不是redis的瓶颈
2.redis是IO操作密集的框架，会有大量的网路IO和磁盘IO
3.多线程虽然能够提高IO利用率，同样会带来共享资源引发的线程安全问题
4.单线程模型，可维护性更高，开发、调试、维护成本低，也避免线程间切换上下文带来的性能开销
5.在单线程中使用多路复用IO技术同样能够提高IO利用率
6.Redis并不是完全单线程的，只有网络IO和键值读写是单线程的
7.持久化模块、集群支持模块等是多线程的

12.消息中间件
顺序性：全局顺序、局部顺序
全局顺序
单一生产者有序将消息投递到同一先进先出队列，单一消费者消费单一先进先出队列
1.kafka一个topic，一个分区，一个消费者

局部顺序
1.生产者将需要有序的数据，顺序投递到消息中间件同一先进先出队列
2.消费者单一线程消费单一先进先出队列

消息中间件无序消费者如何保证顺序
1.全局生成递增版本，投递消息时附上本次版本和上一次消息版本
2.消费端内存保存全局线程安全最后一次版本
3.使用线程安全的先进先出队列
4.每次消费消息时比对消息中上一次版本是否等于内存中版本
5.相等入队列，跟新版本到内存中
6.单线程消费队列

投递消息失败
重投机制

持久化失败
数据先写入Page Cache中，在定时刷磁盘，减少IO
集群方式保证机器异常写失败

生产者如何保证只生产一次
+ kafka在初始化期间，kafka会给生产者生成一个唯一的ID称为Producer ID或PID。
 PID和序列号与消息捆绑在一起，然后发送给Broker。由于序列号从零开始并且单
 调递增，因此，仅当消息的序列号比该PID/TopicPartition对中最后提交的消
 息正好大1时，Broker才会接受该消息。如果不是这种情况，则Broker认定是生产
 者重新发送该消息。

+ kafka局限
producer重启-重新向broker申请pid
不同的Partition具有不同的主键

kafka分区策略
+ 指明partition的情况下，使用指定的partition

+ 没有指明partition，有key的情况下，将key的hash值与topic的partition数进
行取余得到partition值

+ 既没有指定partition，也没有key的情况下，第一次调用时随机生成一个整数
（后面每次调用在这个整数上自增），将这个值与topic可用的partition数取余
得到partition值，也就是常说的round-robin算法




消息只消费一次
+ 保证生产者等幂性
雪花算法为消息生成全局ID，维护已投递消息ID映射，已存在只保留一条

+ 保证消费者等幂性
通用层面，消息处理成功后全局ID存入数据库，处理前检测ID是否处理过，如果需要更为严格的控制加上数据库事务
业务层面，乐观锁、悲观锁、内存去重

消费模式：push和pull
push：推模式是服务器端根据用户需要，有目的、按时将用户感兴趣的信息主动发送到用户的客户端
pull：拉模式是客户端主动从服务端获取信息

push优点
1.服务端主动推送给客户端，及时性很高
2.对于用户要求低，方便用户获取需要的信息

push缺点
1.当客户端消费能力低于服务端生产能力，推送大量信息，会导致客户端消息堆积、处理缓慢，甚至奔溃
2.服务端需要维护每次传输状态，以防消息传递失败进行重试
3.针对性差，可能无法满足客户个性化需求

pull优点
1.客户端可以依据自己的消费能力进行消费
2.传输失败不需要重试，反正数据还在服务端
3.针对性强，能满足客户端的个性化需求
4.服务端压力轻

pull缺点
1.实时性相对差
2.拉取消息间隔时间不好设置，太短导致服务器压力过大，间隔时间太长导致消息延迟
解决方案：
间隔时间指数级增长   
长轮训解决方案，没有数据可消费，不直接返回而是等待有数据后才返回  
3.对于客户端用户的要求高，需要对服务器端有所了解

13.tcp与udp
tcp
+ 面向连接
面向连接，是指发送数据之前必须在两端建立连接。建立连接的方法是
“三次握手”，这样能建立可靠的连接。建立连接，是为数据的可靠传输打下了基础

+ 仅支持单播传输
每条TCP传输连接只能有两个端点，只能进行点对点的数据传输，不支持多播和广播传输方式

+ 面向字节流
TCP不像UDP一样那样一个个报文独立地传输，而是在不保留报文边界的情况下以
字节流方式进行传输

+ 可靠传输
对于可靠传输，判断丢包，误码靠的是TCP的段编号以及确认号。TCP为了保证报文
传输的可靠，就给每个包一个序号，同时序号也保证了传送到接收端实体的包的按
序接收。然后接收端实体对已成功收到的字节发回一个相应的确认(ACK)；如果发
送端实体在合理的往返时延(RTT)内未收到确认，那么对应的数据（假设丢失了）
将会被重传。

+ 提供拥塞控制
当网络出现拥塞的时候，TCP能够减小向网络注入数据的速率和数量，缓解拥塞

+ TCP提供全双工通信
TCP允许通信双方的应用程序在任何时候都能发送数据，因为TCP连接的两端都设
有缓存，用来临时存放双向通信的数据。当然，TCP可以立即发送一个数据段，也
可以缓存一段时间以便一次发送更多的数据段（最大的数据段大小取决于MSS）

udp
+ 面向无连接
首先 UDP 是不需要和 TCP一样在发送数据前进行三次握手建立连接的，想发数据
就可以开始发送了。并且也只是数据报文的搬运工，不会对数据报文进行任何拆分
和拼接操作。

具体来说就是：
在发送端，应用层将数据传递给传输层的UDP协议，UDP只会给数据增加一个UDP头
标识下是 UDP 协议，然后就传递给网络层了
在接收端，网络层将数据传递给传输层，UDP只去除IP报文头就传递给应用层，不会任何拼接操作

+ 有单播、多播、广播的功能
UDP 不止支持一对一的传输方式，同样支持一对多，多对多，多对一的方式，
也就是说 UDP 提供了单播，多播，广播的功能。

+ 面向报文
发送方的UDP对应用程序交下来的报文，在添加首部后就向下交付IP层。UDP对
应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。因此，应
用程序必须选择合适大小的报文

+ 不可靠性
首先不可靠性体现在无连接上，通信都不需要建立连接，想发就发，这样的情况肯定不可靠。

并且收到什么数据就传递什么数据，并且也不会备份数据，发送数据也不会关心对方是否已经正确接收到数据了。

再者网络环境时好时坏，但是 UDP 因为没有拥塞控制，一直会以恒定的速度发送
数据。即使网络条件不好，也不会对发送速率进行调整。这样实现的弊端就是在
网络条件不好的情况下可能会导致丢包，但是优点也很明显，在某些实时性要求高
的场景（比如电话会议）就需要使用 UDP 而不是 TCP。

+ 头部开销小，传输数据报文很高效
UDP 头部包含了以下几个数据：
两个十六位的端口号，分别为源端口（可选字段）和目标端口
整个数据报文的长度
整个数据报文的检验和（IPv4 可选 字段），该字段用于发现头部信息和数据中的错误
因此 UDP 的头部开销小，只有八字节，相比 TCP 的至少二十字节要少得多，在传输数据报文时是很高效的


14.进程间通讯
+ 管道( pipe )：管道是一种半双工的通信方式，数据只能单向流动，
而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系

+ 有名管道 (named pipe) ： 有名管道也是半双工的通信方式，但是它允许无亲
缘关系进程间的通信

+ 信号量( semophore ) ： 信号量是一个计数器，可以用来控制多个进程对共享
资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也
访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段

+ 消息队列( message queue ) ： 消息队列是由消息的链表，存放在内核中并由
消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节
流以及缓冲区大小受限等缺点

+ 信号 ( sinal ) ： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生

+ 共享内存( shared memory ) ：共享内存就是映射一段能被其他进程所访问的内
存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的
IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其
他通信机制，如信号两，配合使用，来实现进程间的同步和通信

+ 套接字( socket ) ： 套解口也是一种进程间通信机制，与其他通信机制不
同的是，它可用于不同及其间的进程通信

https://blog.csdn.net/hzlnice/article/details/84591508
https://www.cnblogs.com/mydomain/archive/2010/09/23/1833369.html

15.NIO与多路IO复用

多路IO复用：
+ select

+ poll

+ epoll

https://www.cnblogs.com/zwt1990/p/8821185.html

16.分布式事务
事务是恢复和并发控制的基本单位

特性
+ 原子性（atomicity）：一个事务是一个不可分割的工作单位，事务中包括
的操作要么都做，要么都不做

+ 一致性（consistency）：事务必须是使数据库从一个一致性状态变到另一个
一致性状态。一致性与原子性是密切相关的。

+ 隔离性（isolation）：一个事务的执行不能被其他事务干扰。即一个事务内
部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间
不能互相干扰。

+ 持久性（durability）：持久性也称永久性（permanence），指一个事务一旦
提交，它对数据库中数据的改变就应该是永久性的。接下来的其他操作或故障不应
该对其有任何影响

事务分类
+ 2pc（两段式提交）
+ 3pc（三段式提交）
+ TCC（Try、Confirm、Cancel）
+ 最大努力通知
+ XA
+ 本地消息表（ebay研发出的）
+ 半消息/最终一致性（RocketMQ）


https://www.codercto.com/a/75285.html
https://www.cnblogs.com/cxxjohnson/p/9145548.html


17.kafka
基本概念

分区中的所有副本统称为AR（Assigned Replicas）。所有与leader副本保持一定
程度同步的副本（包括leader）组成ISR（in-sync replicas）。而与leader副本
同步滞后过多的副本（不包括leader），组成OSR（out-sync replicas），所以，
AR = ISR + OSR。在正常情况下，所有的follower副本都应该与leader副本保持一
定程度的同步，即AR = ISR，OSR集合为空

leader副本负责维护和跟踪ISR中所有follower的滞后状态，当follower落后太多
或者长时间没有向leader发起同步请求，leader副本就会认为它出问题了，会把它
从ISR中移除，这时候这个follower就会放入OSR集合中，直到某个时候这个
follower同步跟上了leader，然后这个副本又会被加入到ISR中。此外，当leader
副本挂了，只有ISR中的follower副本才有资格成为leader，OSR中的则没有资格


+ Consumer : 消息和数据的消费者，订阅数据（Topic）并且处理其发布的消息的
进程、代码或服务；

+ Consumer Group : 逻辑概念，对于同一个topic，会广播给不同的group，一个
group中，只有一个consumer可以消费该消息；

+ Broker : 物理概念，Kafka集群中的每个Kafka节点；

+ Topic : 逻辑概念，Kafka消息的类别，对数据进行区分、隔离；

+ Partition : 物理概念，Kafka下数据存储的基本单位，一个Topic数据会被分散
存储在多个Partition中，每个Partition中的消息是有序的；

+ Replication : 同一个Partition可能会有多个Replia，多个Replica之间数据
一般是一样的；

+ Replication Leader : 一个Partition的多个Replica上，需要一个Leader负责
该Partition上与Producer和Consumer交互；

+ Follower：Follower跟随Leader，所有写请求都通过Leader路由，数据变更会广
播给所有Follower，Follower与Leader保持数据同步。如果Leader失效，则从
Follower中选举出一个新的Leader。当Follower与Leader挂掉、卡住或者同步太
慢，leader会把这个follower从“in sync replicas”（ISR）列表中删除，重新
创建一个Follower

+ Replication Manager : 负责管理当前broker所有分区和副本的信息，处理
KafkaController发起的一些请求，副本状态的切换、添加/读取消息等。


ACK应答机制
+ 0:producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一
接收到还没写入磁盘就已经返回，当broker故障时可能丢失数据

+ 1:producer等待leader的ack，partition的leader落盘成功后返回ack，如果
在follower同步成功之前leader故障，那么将会丢失数据

+ -1(all):producer等待broker的ack，partition的leader和ISR里的follower全
部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，
leader发生故障，那么会造成重复数据。（极端情况下也有可能丢数据：ISR中只
有一个Leader时，相当于1的情况）